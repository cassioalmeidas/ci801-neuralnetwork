\documentclass[12pt]{article}

\usepackage{sbc-template}

\usepackage{graphicx,url}

\usepackage[brazil]{babel}   
\usepackage[utf8x]{inputenc}  

     
\sloppy

\title{Evoluindo os pesos de uma Rede Neural \\com Algoritmos Genéticos}

\author{Aurora Trinidad R. Pozo\inst{1}, Davi Azevedo Q. Santos\inst{1}, Derik Evangelista R. Silva\inst{1}}
  

\address{Departamento de Informática -- Universidade Federal do Paraná (UFPR)\\
  Caixa Postal 19081  -- 81531-980 -- Curitiba -- PR -- Brasil
  \email{\{aurora, daqsantos, dersilva\}@inf.ufpr.br}
}

\begin{document} 

\maketitle

\begin{abstract}
  This meta-paper describes the style to be used in articles and short papers
  for SBC conferences. For papers in English, you should add just an abstract
  while for the papers in Portuguese, we also ask for an abstract in
  Portuguese (``resumo''). In both cases, abstracts should not have more than
  10 lines and must be in the first page of the paper.
\end{abstract}
     
\begin{resumo} 
  Este meta-artigo descreve o estilo a ser usado na confeco de artigos e
  resumos de artigos para publicao nos anais das conferncias organizadas
  pela SBC.  solicitada a escrita de resumo e abstract apenas para os artigos
  escritos em portugus. Artigos em ingls devero apresentar apenas abstract.
  Nos dois casos, o autor deve tomar cuidado para que o resumo (e o abstract)
  no ultrapassem 10 linhas cada, sendo que ambos devem estar na primeira
  pgina do artigo.
\end{resumo}















\section{Introdução}

Contextualização do trabalho: 
\begin{itemize}
\item Contexto histórico de IA/Aprendizado de máquina
\item Motivação
\item Objetivos
\item Linha do trabalho
\end{itemize}

\section{Metodologia} \label{sec:metodologia}

\begin{itemize}
\item Desenvolvimento do trabalho
\item Linguagem utilizada
\item Tratamento das bases
\end{itemize}

\subsection{Redes Neurais}
%\section{Redes Neurais}

\par Segundo \cite{kasabov} uma rede neural artifical é um modelo computacional biologicamente inspirado o qual consiste de elementos de processamento (neurônios) e conexões entre eles (pesos) que representam a memória do sistema. O primeiro modelo de neurônio artificial foi proposto por McCulloch e Pitts em 1943. 
\par Uma rede neural é definida por quatro parâmetros:

\begin{enumerate}
	\item Tipo de neurônio: determina as entradas e saídas de um neuronio;
	\item Arquitetura conexionista: determina a arquitetura da rede.
	\item Algoritmo de aprendizado: determina como o conhecimento é armazenado na rede.
	\item Algoritmo de Recall: determina como o conhecimento da rede é recuperado.
\end{enumerate}

\par Os vários modelos de redes neurais artificais podem ser descritos em termos destes parâmetros. O modelo mais comum é o Multi Layer Perceptron (MLP), que consiste de uma arquitetura totalmente conectada entre as camadas, possui no mínimo três camadas (entrada, escondida e saída), cada neurônio tem um entrada fixa (bias), e a função de ativação mais comum é a sigmóide. O MLP tornou-se bastante utilizado com surgimento do algoritmo Backprogation como algoritmo de aprendizado. Contudo neste trabalho, será utilizado um algoritmo genético para realizar o treinamento da rede neural.

\par Neste trabalho utilizaremos o MLP para o problema de classificação. Segundo \cite{kasabov} O problema de classificacão é associar um objeto a um grupo ou classe de objetos já existentes. 

\subsection{Algoritmos Genéticos}
%\section{Algoritmos Genéticos}

\par O Algoritmo Genético (AG), geralmente referenciado como \emph{algoritmos genéticos}, foi desenvolvido por John Holland na Universidade de Michigan, em 1975, em um dos livros mais famosos da área, \emph{Adaptation in Natural and Artificial Systems}, publicado pela editora \emph{University of Michigan Press} \cite{essentials:pop}.
\par Os AGs são algoritmos de busca heurística que tentam reproduzir artificialmente o processo de evolução, da Teoria de Evolução das Espécies, de Darwin. São muito similares a outros algoritmos de busca local, como o Subida de Encosta (\textit{Hill-climbing}), diferindo apenas na quantidade de soluções mantidas a cada iteração e no processo de geração de novas soluções.
\par De acordo com \cite{montana}, os AGs necessitam de cinco componentes:
\begin{enumerate}
\item[C1] - Uma maneira de codificar uma solução em um indivíduo.
\item[C2] - Uma função de avaliação que retorna um índice de qualidade para cada individuo da população.
\item[C3] - Um procedimento de inicialização da população.
\item[C4] - Operadores que podem ser aplicados nos indivíduos pais no processo de reprodução, alterado a composição genética dos novos indivíduos gerados. Neste componente incluem-se os operadores de mutação, de cruzamento e outros específicos do domínio.
\item[C5] - Uma configuração de parâmetros do algoritmo, os operadores, etc.
\end{enumerate}
Com estes cinco componentes, ainda de acordo com \cite{montana}, o AG funciona seguindo os seguintes passos:
\begin{enumerate}
	\item A população é iniciada usando-se o procedimento C3. O resultado é um conjunto de indivíduos, de acordo com C1.
	\item Cada indivíduo é avaliado, usando a função definida em C2.
	\item A população se reproduz até que um critério de parada seja atingido. A reprodução é realizada seguindo-se os seguintes passos:
	 \begin{enumerate}
		\item Um ou mais indivíduis são escolhidos para a reprodução. A seleção é estocástica, mas os pais melhores avaliados são favorecidos na escolha. Os parametros em C5 podem influenciar neste processo de seleção.
		\item Os operadores de C4 são aplicados aos pais para geração dos filhos. Os parametros em C5 ajudam a determinar quais operadores serão usados.
		\item Os filhos são então avaliados e inseridos de volta na população. Em algumas versões de AG, toda a população é substituida. Em outras, apenas um subconjunto é substituido.		
	\end{enumerate}
\end{enumerate}

\par A cada iteração, os indivíduos melhor avaliados tem mais chances de serem escolhidos para reprodução, fazendo com que, em teoria, sejam gerados melhores indivíduos a cada geração. Ao término do algoritmo, senão a ótima, a tendência é ter-se uma solução muito próxima a ela.


\subsection{Bases de dados}

\par As bases de dados utilizadas foram retiradas de \cite{frank}. Para o experimento foram utilizadas as seguintes bases:

\begin{enumerate}
	\item \textit{Breast Cancer Wisconsin (Original) Data Set}, doravante denominada \textit{Câncer};
	\item \textit{Pima Indians Diabetes Data Set};
	\item \textit{Glass Identification Data Set};
	\item Statlog (Heart) Data Set, doravante denominada \textit{Heart};
	\item \textit{Iris Data Set}.
\end{enumerate}


\section{Implementação}\label{sec:imple}

\par A linguagem Java foi utilizada para implementar os algoritmos. A rede neural foi codificada como um vetor de números em ponto flutuante, também conhecido como \textit{real-coded} \cite{Liu}. A figura \ref{fig:nn} ilustra esse processo, que foi baseado em \cite{montana}. Desta forma, é possível determinar rapidamente quais são os pesos de entrada e saída de cada neurônio e assim facilitar a implementação dos o\-pe\-ra\-do\-res genéticos. O número de camadas da rede é fixo e consiste em uma camada de entrada, uma camada oculta e uma camada de saída. Na tabela \ref{tab:params}, temos o número de neurônios de cada camada para cada base de dados a ser testada.

\begin{table}[h]
	\center
	\begin{tabular}{|c|c|c|c|c|}
		\hline Base  & Atributos (Entradas) & Classes (Saídas) & Escondidos \\ 
		\hline Cancer & 9 & 2 & 5 \\ 
		\hline Pima Indians Diabetes & 8 & 2 & 10 \\ 
		\hline Glass Identification & 9 & 7 & 10 \\ 
		\hline Heart & 13 & 2 & 5 \\ 
		\hline Iris & 4 & 3 & 10 \\ 
		\hline 
	\end{tabular}
	\caption{Número de neurônios de cada camada}
	\label{tab:params}
\end{table} 

\begin{figure}[ht]
\centering
\includegraphics[width=80mm]{codificacao.png}
\caption{Codificacão da rede}
\label{fig:nn}
\end{figure}

\par O algoritmo genético foi implementado conforme \cite{essentials}. O \textit{fitness} de cada indivíduo é dado pelo Erro Quadrado Médio (EQM), ou seja, um indivíduo mais apto é aquele que possui o menor valor. O EQM é calculado de acordo com \cite{Liu}.


\par Os operadores genéticos utilizados foram:
\begin{enumerate}
	\item Operadores de Mutação, segundo \cite{montana} e \cite{Liu}:
	\begin{enumerate}
		\item Biased Mutate Weights: O valor de um gene pode ser substituído por um outro qualquer da distribuição de probabilidade inicial (distribuição normal com média 0 e desvio padrão 5).
		\item Unbiased Mutate Weights: Um valor da distribuição de probabilidade inicial é acrescido a um gene.
		\item Mutate Nodes: Este operador seleciona n neurônios das camadas oculta e de saída e adiciona um valor da distribuição de probabilidade inicial a cada peso de entrada dos neurônios. Nos nossos o valor de n foi 2.
		\item Mutate Weakest Nodes: Este operador seleciona o neurônio mais fraco (aquele que menos contribui para o saída da rede) e aplica uma mutação \textit{unbiased} ou \textit{biased} em cada peso do neurônio mais fraco.
		\item SinglePointRandom: Cada gene do cromossomo é substituído por um valor aleatório entre [-50, 50].
		\item NonUniform: implementado conforme \cite{Michalewicz}. Na nossa implementacão o parâmetro b é 4.
		
	\end{enumerate}

	\item Operadores de Crossover, segundo \cite{montana} e \cite{Liu}:
	\begin{enumerate}
		\item Crossover Weights: O valor gene de um indivíduo filho é escolhido pela seleção aleatória do mesmo gene de um dos pais.
		\item Crossover Nodes: Para cada neurônio de um dos pais escolhidos aleatoriamente os pesos associados são passados diretamente para o filho.
		\item Crossover Features: Para cada neurônio da rede do primeiro pai, o algoritmo reorganiza o segundo pai de forma que os neurônios que desempenham o mesmo papel (isto é, possum a mesma saída para uma determinada entrada) fiquem na posição, assim formando um pai intermediário. Depois disso é aplicado o operador Crossover-Nodes entre o primeiro pai e o pai intermediário.
		\item Line Recombination: A implementação foi um hibrido entre \cite{Liu} e \cite{essentials} onde o parâmetro é dado por uma distribuição normal com média 0 e desvio padrão 5.
	\end{enumerate}

\end{enumerate}

\par Os parâmetros utilizados para o Algoritmo Genético foram os seguintes:
\begin{enumerate}
	\item Tamanho da População: 50 e 100 indivíduos;
	\item Número de Gerações: 500;
	\item Tamanho do torneio: 2 e 4;
	\item Número de indivíduos da elite: 10;
	\item Probabilidade de Crossover: 100\%;
	\item Probabilidade de mutação: 10\%.
\end{enumerate}

\par Para a análise dos resultados foi utilizado a validação cruazada \cite{Haykin:1998:NNC:521706}. Na validação cruzada o conjunto de dados é particionado aleatoriamente em um conjunto de teste e um conjunto de treinamento, que ainda é divido em um subconjunto de treinamento e um subconjunto de validação. O conjunto de dados foi separado de acordo com a tabela \ref{tab:db}.

\begin{table}
\center
\begin{tabular}{|c|c|c|c|c|}
\hline Base & Treinamento & Validação & Teste  & Total \\ 
\hline Cancer & 350 & 175 & 174 & 699 \\ 
\hline Pima Indians Diabetes & 252 & 258 & 258 & 768 \\ 
\hline Glass Identification & 114 & 50 & 50 & 214 \\ 
\hline Heart & 130 & 70 & 70 & 270 \\ 
\hline Iris & 70 & 40 & 40 & 150 \\ 
\hline 
\end{tabular} 
\caption{Separação dos conjuntos de dados}
\label{tab:db}
\end{table}


\section{Resultados obtidos}

Foram realizadas 10 execuções para cada combinação possível de:
\begin{enumerate}
\item Parâmetros do algoritmo genético (explicitados na seção \ref{sec:imple});
\item Operadores de mutação;
\item Operadores de cruzamento;
\end{enumerate}

Para os operadores de mutação e cruzamento, considerou-se também um operador extra, que seleciona aleatoriamente qualquer um dos operadores descritos.

\subsection{Influência dos parâmetros do AG}\label{res:ag}

Um tamanho de torneio maior, mostrou, de uma maneira geral, ter um impacto positivo na média dos valores de fitness. Entretanto, como podemos ver nas figuras \ref{fig:can100.2} e \ref{fig:can100.4}, a média piora em alguns casos e, nos casos em que há melhora, ela é muito pequena, como podemos ver nas figuras \ref{fig:gla50.2} e \ref{fig:gla50.4}.

\begin{figure}[htp]
\center
\includegraphics[scale=0.4, keepaspectratio]{cancer_100_2.jpg} 
\caption{Fitness - Câncer; População 100, torneio 2;}
\label{fig:can100.2}
\end{figure}

\begin{figure}[hbp]
\center
\includegraphics[scale=0.4, keepaspectratio]{cancer_100_4_converted.jpg} 
\caption{Fitness - Câncer; População 100, torneio 4;}
\label{fig:can100.4}
\end{figure}

\begin{figure}[htp]
\center
\includegraphics[scale=0.4, keepaspectratio]{glass_50_2.jpg} 
\caption{Fitness - Glass; População 50, torneio 2;}
\label{fig:gla50.2}
\end{figure}

\begin{figure}[hbp]
\center
\includegraphics[scale=0.4, keepaspectratio]{glass_50_4.jpg} 
\caption{Fitness - Glass; População 50, torneio 4;}
\label{fig:gla50.4}
\end{figure}

Já um maior tamanho da população apresenta uma influência bem mais significativa que o tamanho do torneito em praticamente todas as execuções. Podemos notar essa diferença analisando as figuras \ref{fig:dia50.4} e \ref{fig:dia100.4}. Entretanto, ainda assim, a relação de consumo de recursos computacionais, associado ao maior tempo para execução e o ganho real de uma população maior pode não ser vantajoso.

\begin{figure}[htp]
\center
\includegraphics[scale=0.4, keepaspectratio]{dia_50_4.jpg} 
\caption{Fitness - Diabetes; População 50, torneio 4;}
\label{fig:dia50.4}
\end{figure}

\begin{figure}[hbp]
\center
\includegraphics[scale=0.4, keepaspectratio]{dia_100_4.jpg} 
\caption{Fitness - Diabetes; População 100, torneio 4;}
\label{fig:dia100.4}
\end{figure}

\begin{figure}[htp]
\center
\includegraphics[scale=0.4, keepaspectratio]{hea_100_4.jpg} 
\caption{Fitness - Heart; População 100, torneio 4;}
\label{fig:hea100.4}
\end{figure}

\begin{figure}[hbp]
\center
\includegraphics[scale=0.4, keepaspectratio]{iri_100_2.jpg} 
\caption{Fitness - Iris; População 100, torneio 2;}
\label{fig:iri100.2}
\end{figure}

\subsection{Influência dos Operadores Genéticos}

Diferentemente dos parâmetros em \ref{res:ag}, a escolha certa dos operadores genéticos tem um impacto muito grande. Podemos observar facilmente este impacto analisando a figura \ref{fig:hea100.4}. Podemos notar uma grande amplitude entre a pior combinação de operadores, que termina com a média de fitness um pouco maior que 0.15 e a melhor combinação, que termina com um valor de fitness menor que 0.05.

Também é interessante notar que em algumas casos, como o mostrado nas figuras \ref{fig:dia50.4} e \ref{fig:iri100.2}, muitas combinações de operadores mantem os valores de fitness muito próximos entre si e, como visto na figura \ref{fig:dia100.4}, há uma tendência a, em algum ponto, estabilizar, passando muitas gerações sem uma melhora sensível no valor. Estas características encontradas podem ser, em partes, remediadas aplicando-se, tal qual proposto por \cite{montana}, uma espécie de torneio com os operadores, dando preferência para aqueles que estão produzindo um maior impacto na rede.

\subsection{Taxas de Acerto}

Ao término das execuções, conseguimos analisar qual a taxa de acerto média da rede neural com seus pesos evoluidos através de um algoritmo genético.

Na tabela \ref{tab:hitbest}, podemos visualizar as melhores taxas de acerto, tanto para o conjunto de testes, quanto para o conjunto de validação, assim como qual combinação de parâmetros e operadores maximizou o acerto. É interessante notar que apenas dois (dos cinco possíveis) operadores de cruzamento aparecem (Line Recombination e Aleatório). Já os operadores de mutação são mais heterogêneos. Podemos concluir, então, que o cruzamento tem um efeito maior na saída da rede que a mutação. Entretanto, se considerarmos que a mutação ocorre apenas em 10\% das vezes, ao passo que o cruzamento ocorre sempre, este comportamento é esperado.

Também podemos perceber que, em alguns casos, a rede se comportou melhor com uma população e torneio menores, embora na maior parte das execuções uma grande população e um maior torneio tenham superado as outras combinações.

Ao compararmos com a média de acerto para estas bases, na tabela \ref{tab:hitaim}, à excessão da base Câncer, o desempenho da rede foi bem abaixo do esperado, principalmente nas bases Glass e Iris. Analisando as figuras \ref{fig:gla50.4} e \ref{fig:iri100.2}, percebemos que o fitness para estas bases converge rapidamente para um mínimo local e estaguina, não conseguindo sair deste mínimo. Por esse motivo, os valores encontrados para estas bases fica tão distante do esperado.

\begin{table}[h]
\center
\begin{tabular}{c|c|c|p{3cm}|p{3cm}|c|c|}
\cline{2-7}
&  \multicolumn{4}{|c|}{Parâmetros}  & \multicolumn{2}{|c|}{Taxas} \\ \hline
\multicolumn{1}{|c|}{Base} & Pop & Tor & Mutação & Cruzamento & Teste & Validação \\ \hline
\multicolumn{1}{|c|}{Câncer} & 100 & 2 & Single Point Random & Aleatório & 0.964 & \textbf{0.954022 }\\ \hline
\multicolumn{1}{|c|}{Câncer} & 50 & 2 & Aleatório & Line Recombination & \textbf{0.961142 }& 0.959770 \\ \hline
\multicolumn{1}{|c|}{Diabetes} & 100 & 4 & Single Point Random & Line Recombination & 0.695348 & \textbf{0.690310}\\ \hline
\multicolumn{1}{|c|}{Diabetes} & 100 & 2 & Unbiased Mutate Weights & Line Recombination & \textbf{0.663953} & 0.692635 \\ \hline
\multicolumn{1}{|c|}{Glass} & 100 & 4 & NonUniform & Aleatório & 0.414 & \textbf{0.426 }\\ \hline 
\multicolumn{1}{|c|}{Glass} & 100 & 4 & NonUniform & Aleatório & \textbf{0.414} & 0.426 \\ \hline 
\multicolumn{1}{|c|}{Heart} & 50 & 2 & Unbiased Mutate Weights & Line Recombination & 0.795714 & \textbf{0.774285} \\ \hline
\multicolumn{1}{|c|}{Heart} & 100 & 4 & Single Point Random & Line Recombination & \textbf{0.784285} & 0.788571 \\ \hline
\multicolumn{1}{|c|}{Iris} & 100 & 4 & NonUniform & Aleatório & 0.414 & \textbf{0.426 }\\ \hline 
\multicolumn{1}{|c|}{Iris} & 100 & 4 & NonUniform & Aleatório & \textbf{0.414} & 0.426 \\ \hline
\end{tabular}
\caption{Melhores taxas de acerto}
\label{tab:hitbest}
\end{table}


\begin{table}[ht]
\begin{minipage}[b]{0.5\linewidth}\centering
\begin{tabular}{|c|c|}
\hline
Base & Acerto esperado \\ \hline
Câncer & 0.96 \\ \hline
Diabetes & 0.77 \\ \hline
Glass & 0.63 \\ \hline
Heart & 0.82 \\ \hline
Iris & 0.96 \\ \hline
\end{tabular}
\caption{Taxas de acerto esperado}
\label{tab:hitaim}
\end{minipage}
\hspace{0.3cm}
\begin{minipage}[b]{0.5\linewidth}
\centering
\begin{tabular}{|c|c|c|}
\hline 
Base & Teste & Validação \\ \hline
Câncer & 0.914048 & 0.913756 \\ \hline
Diabetes & 0.619648 & 0.620949 \\ \hline
Glass & 0.1881 & 0.189014 \\ \hline
Heart & 0.694551 & 0.695867 \\ \hline
Iris & 0.1881 & 0.189014 \\ \hline
\end{tabular}
\caption{Média das taxas de acerto}
\label{tab:hitmed}
\end{minipage}
\end{table}

\section{Considerações Finais}

Baseado na análise dos resultados obtidos, podemos notar que, apesar do algoritmo conseguir uma rápida queda nos valores de fitness em poucas gerações, o que aumenta as taxas de acerto da rede, a evolução dos pesos da rede sofre com o problema recorrente aos AGs, de otimização fina. Ao chegar em um certo valor de fitness, por mais que se alterem os códigos genéticos dos filhos, é muito difícil continuar a melhorar a rede, causando uma estagnação dos valores. É provavel que, mesmo que aumentemos o número de gerações, a melhora da rede seja ínfima.

Para evitar isto, se faz necessário a criação de novos (e melhores) operadores genéticos, que consigam impactar a rede de forma que os pesos continuem a evoluir. Além disso, desenvolver um mecanismo de evolução dos operadores, conforme descrito por \cite{montana}, de forma que os operadores sejam escolhidos de acordo com o impacto que este teve anteriormente na rede, pode melhorar ainda mais a evolução dos pesos. Isto pode ser feito em trabalhos futuros.

\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}
